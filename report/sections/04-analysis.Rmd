---
output: pdf_document
---

# Analysis

## Exploratory Data Analysis (EDA)

The first step of conducting analysis is to understand the data by conducting exploratory data analysis. To conduct the EDA, we obtained descriptive statistics and summaries of all variables. For the quantitative variables, we wrote a function called output_quantitative_stats() to get minimum, maximum, range, median, first and third quartiles, IQR, Mean and Sd of all the quantitative variables including "UGDS_BLACK", "UGDS_HISP", "UGDS_ASIAN", "UGDS_ASIAN", "UGDS_AIAN", "UGDS_NHPI", "UGDS_2MOR","UGDS_NRA", "UGDS_UNKN","UGDS_WHITE", "UGDS", "ADM_RATE", "COSTT4_A", "MD_EARN_WNE_P10", "C100_4", "PCTFLOAN", "CIP_SUM", "MINORATIO", and "STU_APPLIED". 

Similarly, we wrote a function called output_qualitative_stats() to generate a table with both the frequency and the relative frequency of the qualitative variables including "WEST", "MIDWEST", "NORTHEAST", "SOUTH", "MAJOR_CITY", "MINOQ1", "MINOQ2", "MINOQ3",and "MINOQ4". To understand the data better, we also want to generate some plots to visualize the data. We wrote the functions histogram_generator() and boxplot_generator() to generate histograms and boxplots of the quantitative variables and condition_boxplot_generator() to generate conditional boxplots between "STU_APPLIED" and the qualitative variables. To study the association between "STU_APPLIED" and the rest of predictors, we also obtained the correlation matrix of all quantitative variables using function cor(), the scatterplot matrix using function pairs(), the anova between "STU_APPLIED" and all the qualitative variables using function aov().

To further analyze the data, we divided the data set into 8 separate clusters  

```{r}
library(xtable)
library(Matrix)
load("../../data/data-outputs/anova.RData")
```

After dividing our data set into 8 separate clusters, our first step is to perform an analysis of variance. ANOVA is designed to test whether there are any statistically significant differences between the means of independent groups. Since our measure of school competitiveness is the number of students applied, we will test whether the means of students applied is different among clusters to gauge whether our clustering criteria makes sense. 

```{r results= 'asis', echo = FALSE}
print(xtable(summary(aov_result), caption = 'ANOVA Test Result'), comment = FALSE)
```

The test result shows that we have a p-value smaller than 0.01, which means that we can reject the null hypothesis that the means are the same across all 8 clusters. 

Then, in order to develop strategies to improve competitiveness for each cluster, we first tabulate some key statistics for each cluster.




To start with, we run an OLS regression for all variables that we believe have an impact on number of students applied. 

$Students applied = Median_Earning + Completion_rate + Percentage_with_Student_Loans + Major_City + Minority_Ratio + West + Midwest + Northeast$

The dependent variable is number of students applied in the fall term. It is calculated by dividing people admitted during fall term by the admission rate.

We use 6 main variables as regressors in the regression:

1. Median_Earning: Student's median earning 10 years after graduation

2. Completion_rate: Percentage of students graduated within 4 years. 

3. Percentage_with_Student_Loans: Percentage of students with Student Loans.

4. Major_City: Whether the institution is located near a major city or countryside. This dummy variable equals 1 if it is located in a major city, 0 if countryside. 

5. Minority_Ratio: THe ratio of non-white students to the total population.

6. West, Midwest, East: Region dummy variables. We divided all schools into 4 regions and if an institution belongs to a certain region, the corresponding dummy varible will be 1, and others be 0. We drop one dummy variable Northeast in the multiple linear regression to avoid perfect collinearity. 


```{r results= 'asis', echo = FALSE}
load("../../data/regression-data/overall-ols-model.RData")
print(xtable(summary(ols_full_fit), caption = 'OLS Regression Output for the Full Data Set'), comment = FALSE)
```

We noticed that the coefficients are large and each variable comes with different units. In order to make the regression result more intepretable, we take logs of all the quantitative data in the regression. Because we can take log of 0, we replace 0 with 0.0001 in our data set. 

In addition, since our data set is large enough, we will first split the set into train set and test set in order to gauge our the performance of our estimated coefficients. We divide the data set according to 3:1 train and test ratio. The train set is used to build the model and test set calculate the SE. 

```{r results= 'asis', echo = FALSE}
print(xtable(summary(ols_full_log_fit), caption = 'OLS Regression Output After Taking Log'), comment = FALSE)
```

Based on the regression output, some findings match with our expectations:

1. For every 1% increase in median earning 10 years after graduation, we expect the number of students applied to  increase by 2.9%, holding other varaibles constant. 

2. Every 1% increase in percentage of students with student loans is associated with a 0.36% decrease in number of students applied, holding other variables constant. 

3. If the 4-year completion rate goes up by 1%, the number of students applied is predicted to increase by 0.28%, holding other varaibles constant. 

4. If cost of attendence increase by 1%, on average, we expect the number of students applied decrease by 1.33%, holding other variables constant.

5. If an institution is located near a major city, the number of students applied will be 0.16% higher than schools in countryside, holding other variables constant.

6. If the minority ratio increases by 1%, we predict the number of students applied will increase by 0.98%. 

Those 6 coefficients are all very significant at 1% significance level with p-value close to 1. 

Since OLS is the most common and versatile method, it is our first choice. However, in order to decide which method fits our data better, we also apply ridge and lasso regression method. 

```{r results= 'asis', echo = FALSE}
load("../../data/regression-data/overall-ridge-model.RData")
load("../../data/regression-data/overall-lasso-model.RData")
reg_coef_mat = cbind(ols_fitted_coef, as.numeric(ridge_fitted_coef), as.numeric(lasso_fitted_coef))
colnames(reg_coef_mat) = c("ols", "ridge", "lasso")
print(xtable(reg_coef_mat, caption = 'Regression Coefficients for 3 Regression Methods'), comment = FALSE)
```

For Ridge and Lasso, because of the way the regression is set up, SE are not reliable measures. But comparing all the coefficients, we can see that the effect of earnings become smaller while the effect of locating at a major city significantly increase. 