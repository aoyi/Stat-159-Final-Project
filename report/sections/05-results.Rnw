\section{Results}
\subsection{overall regression results}
<<eda_stats, echo = FALSE, results=tex>>=
load("../data/regression-data/overall-ols-model.RData")
print(xtable(summary(ols_full_fit), caption = 'OLS Regression Output for the Full Data Set'), comment = FALSE)
@

We noticed that the coefficients are large and each variable comes with different units. In order to make the regression result more interpretable, we take logs of all the quantitative data in the regression. Because we can take log of 0, we replace 0 with 0.0001 in our data set.\\

In addition, since our data set is large enough, we will first split the set into train set and test set in order to gauge our the performance of our estimated coefficients. We divide the data set according to 3:1 train and test ratio. The train set is used to build the model and test set to calculate the SE.\\

<<eda_stats, echo = FALSE, results=tex>>=
print(xtable(summary(ols_full_log_fit), caption = 'OLS Regression Output After Taking Log'), comment = FALSE)
@

Based on the regression output, some findings match with our expectations:
\begin{enumerate}
  \item For every 1\% increase in median earning 10 years after graduation, we expect the number of students applied to  increase by \Sexpr{round(summary(ols_full_log_fit)$coefficients[2, 1], 2)}\%, holding other variables constant.

  \item Every 1\% increase in percentage of students with student loans is associated with a \Sexpr{-round(summary(ols_full_log_fit)$coefficients[3, 1], 2)} decrease in number of students applied, holding other variables constant.

  \item If the 4-year completion rate goes up by 1\%, the number of students applied is predicted to increase by \Sexpr{round(summary(ols_full_log_fit)$coefficients[4, 1], 2)}\%, holding other variables constant.

  \item If cost of attendence increase by 1\%, on average, we expect the number of students applied decrease by \Sexpr{-round(summary(ols_full_log_fit)$coefficients[5, 1], 2)}\%, holding other variables constant.

  \item If an institution is located near a major city, the number of students applied will be \Sexpr{round(summary(ols_full_log_fit)$coefficients[6, 1], 2)}\% higher than schools in countryside, holding other variables constant.

  \item If the minority ratio increases by 1\%, we predict the number of students applied will increase by \Sexpr{-round(summary(ols_full_log_fit)$coefficients[7, 1], 2)}\%.
\end{enumerate}

Those 6 coefficients are all very significant at 1\% significance level with p-value close to 0.\\

Since OLS is the most common and versatile method, it is our first choice. However, in order to decide which method fits our data better, we also apply Ridge regression (RR), Lasso regression (LR), Principal Components regression (PCR) and Partial Least Squares regression (PLSR) method.\\

In order to improve our accuracy on predicting MSE, we use cross validation. We used sample() function to get a 3:1 train test split of our original data and for reproducibility purpose, we set.seed() before running the simulation.\\

For ridge and lasso regression method, we used cv.glmnet() in R package "glmnet" to conduct the ten-fold cross-validation on the train set. We then used the best fitted lambda we found from the train set to build a model and calculate MSE from the test set in order to gauge our performance.\\

Similarly, for pcr and plsr regression method, we used function pcr() and plsr() in "pls" package to perform the 10-fold cross-validation. We also use the best fitted m from the train set to build the model and obtain the MSE from the test set.\\


<<eda_stats, echo = FALSE, results=tex>>=
load("../data/regression-data/overall-ridge-model.RData")
load("../data/regression-data/overall-lasso-model.RData")
load("../data/regression-data/overall-pcr-model.RData")
load("../data/regression-data/overall-plsr-model.RData")

reg_coef_mat = cbind(ols_fitted_coef, as.numeric(ridge_fitted_coef), as.numeric(lasso_fitted_coef), c(NA,pcr_fitted_coef), c(NA,plsr_fitted_coef))
reg_coef_mat = reg_coef_mat[-1,]
colnames(reg_coef_mat) = c("ols", "ridge", "lasso", "pcr", "plsr")
print(xtable(reg_coef_mat, caption = 'Regression Coefficients for 5 Regression Methods', digits = c(0,5,5,5,5,5)), comment = FALSE)

MSE_val = as.matrix(c("ols" = MSE_ols, "ridge" = MSE_ridge, "lasso" = MSE_lasso, "pcr" = MSE_pcr, "plsr" = MSE_plsr))
colnames(MSE_val) = "MSE"
print(xtable(MSE_val, caption = 'MSE of 5 Regression Methods', digits = 5), comment = FALSE)

par(mfrow = c(1,1))
plot(reg_coef_mat[,1], xlab = "Coefficients", ylab = "Value", main = "Trend Lines of Coefficients for Different Regression Models")
lines(reg_coef_mat[,1])
points(reg_coef_mat[,2],col= "blue")
lines(reg_coef_mat[,2],col = "blue")
points(reg_coef_mat[,3], col = "red")
lines(reg_coef_mat[,3],col = "red")
points(reg_coef_mat[,4], col = "green")
lines(reg_coef_mat[,4],col = "green")
points(reg_coef_mat[,5], col = "yellow")
lines(reg_coef_mat[,5],col = "yellow")
legend(7.3,3,c('OLS','Ridge','Lasso','PCR','PLSR'), lty = c(1,1,1,1,1), lwd = c(2.5,2.5,2.5,2.5,2.5), col = c("black","blue","red","green","yellow"),merge = T)
abline(h = 0, lty = 3)
@

We notice that all regression methods have similar MSE with lasso resulting the smallest. After interpreting the coefficients, we decide to use ols on the regression that we will run for each cluster for two main reasons:
\begin{enumerate}
  \item lm() provides us with a p-value associated with each coefficient. This gives us information regarding whether the impact of a certain variable is significant which plays a vital role in determining our advice to schools in a certain cluster. In contrast, due to the way Ridge and Lasso regression are designed, although they give out better prediction, but the SE associated with each coefficient is unreliable, so we will lose this information if we go with those regression methods.

  \item We notice that some of our regression variables are correlated. Therefore, ridge and lasso regression, aiming to reduce the dimension by eliminating unnecessary variables, can cause a problem. If both variables can affect the school competitiveness through the same channel, we don't want the regression randomly drop one since they explain the same portion of change in Y. Instead, we want to make the decision on our own based on the budget required for each change or whether it is practical to execute for a certain institution.
\end{enumerate}
Therefore, weighing all the pros and cons for each regression method, we decide to use OLS to run the regression for each cluster.\\

\subsection{cluster regression results}
Here is a summary of all the coefficients and our advice for institutions in different areas.\\

<<eda_stats, echo = FALSE, results=tex>>=
  load("../data/regression-data/WM-ols-model.RData")
print(xtable(summary(WM_model)$coefficients, caption = 'Regression Coefficients WM Cluster'), comment = FALSE)
@
\begin{enumerate}  
\item\textbf{For schools located in the West major cities}:
  
All the coefficients are significant except minority ratio. This can be largely explained by the fact that this cluster has the highest average minority ratio among all clusters, meaning those institutions already have a very diverse student population. Therefore, further improving this aspect won't have a significant impact on the school's competitiveness.\\

The most significant regressors is median earnings 10 years after graduation. Most cities on the West Coast are where well-paid technology companies and banking industry clutered. Therefore, there is a possibility that people decide to attend a university on the West Coast in order to get a job with high pay. So during the application process, it is reasonable if they pay extra attention to their future career development and the salary level of previous graduates serve as a plausible measure.\\

<<eda_stats, echo = FALSE, results=tex>>=
  load("../data/regression-data/WN-ols-model.RData")
print(xtable(summary(WN_model)$coefficients, caption = 'Regression Coefficients WN Cluster'), comment = FALSE)
@
  
\item\textbf{ For schools located on the West coast countryside:}
  
This cluster is our smallest one, so the coefficient might not be as accurate as clusters with larger population. However, from the EDA stage, we notice that WN cluster has the lowest average percentage of students with loans. This perfectly explains the fact that a 1\% increase in this percentage won't have a significant effect on school competitiveness because their current level of students with debt is very low.\\

The most significant factor here is similar to that of group WM, which is median earnings. And this finding shows that there exists similarities between schools within the same region, possibly due to local culture and regional economic development.\\

<<eda_stats, echo = FALSE, results=tex>>=
load("../data/regression-data/MM-ols-model.RData")
print(xtable(summary(MM_model)$coefficients, caption = 'Regression Coefficients MM Cluster'), comment = FALSE)
load("../data/regression-data/MN-ols-model.RData")
print(xtable(summary(MN_model)$coefficients, caption = 'Regression Coefficients MN Cluster'), comment = FALSE)
@

\item\textbf{ For schools located in the Midwest major cities and countryside:}

Those two clusters generate very similar results with all the coefficients being significant, especially MN cluster. According to the EDA, MN is the region with the lowest average number of students applied. Therefore, it makes sense for all the varaibles to show statistical significance since they have a lot of room for improvement. The most significant coefficient in both clusters, median earning, can be interpreted as if we can increase the earnings after graduation by 1\%, we predict the students applied will increase by \Sexpr{round(summary(MM_model)$coefficients[2, 1], 2)}\% and \Sexpr{ round(summary(MN_model)$coefficients[2, 1], 2)}\% in schools located in the midwest cities and countryside, holding all other variables constant. In addition, they have relative low minority ratio among all clusters, therefore boosting their minority ratio by 1\% is expected to bring an additional \Sexpr{round(summary(MM_model)$coefficients[6, 1], 2)}\% and \Sexpr{round(summary(MN_model)$coefficients[6, 1], 2)}\% in MM and MN cluster respectively, holding all other variables constant.\\


<<eda_stats, echo = FALSE, results=tex>>=
load("../data/regression-data/NM-ols-model.RData")
print(xtable(summary(NM_model)$coefficients, caption = 'Regression Coefficients NM Cluster'), comment = FALSE)
@

\item\textbf{ For schools located in the northeast major cities:}

All the coefficients are significant besides that of graduation rate. This cluster has the highest 4-year graduation rate, which explains why an additional boost in this rate won't bring as much increase in school competitiveness as other variables. Similarly to the West region, Northeast has prosperous economy and harbour millions of high-tech and finance related-companies. Therefore, it is not surprising to see that the effect of pay after graduation is has a t-value that is significantly higher than other factors, and we expect a 1\% increase in graduation pay is associated with a \Sexpr{round(summary(NM_model)$coefficients[2, 1], 2)}\% increase in number of students applied, holding all other variables constant.\\

<<eda_stats, echo = FALSE, results=tex>>=
  load("../data/regression-data/NN-ols-model.RData")
print(xtable(summary(NN_model)$coefficients, caption = 'Regression Coefficients NN Cluster'), comment = FALSE)
@
  
\item\textbf{ For schools located in the northeast countryside:}
  
With a relatively low minority ratio to start with, improving minority ratio will bring the largest increase in school competitiveness. For each 1\% increase in minority ratio, we expect the number of students applied will increase by \Sexpr{round(summary(NN_model)$coefficients[2, 1], 2)}\%. Similarly, due to the regional effect we mentioned for the northeast area, we also have a large value of coefficient for median earning. Each 1\% increase in median earning is expected to add \Sexpr{round(summary(NN_model)$coefficients[6, 1], 2)}\% in number of students applied, holding all other variables constant.\\

<<eda_stats, echo = FALSE, results=tex>>=
  load("../data/regression-data/SM-ols-model.RData")
print(xtable(summary(SM_model)$coefficients, caption = 'Regression Coefficients SM Cluster'), comment = FALSE)
load("../data/regression-data/SN-ols-model.RData")
print(xtable(summary(SN_model)$coefficients, caption = 'Regression Coefficients SN Cluster'), comment = FALSE)
@
  
\item\textbf{For schools located in the south major cities and countryside:}
  
SM and SN have similar statistics in terms of graduation rate and the number of students with student loans.\\

This cluster has the lowest average of number of students applied and median earnings among all clusters with the second highest coast of attendence. Therefore, all the coefficients are significant with coefficients of median earnings and cost of attendence. If the school can implement programs to improve the earnings of their graduates by 1\%, we expect there will be \Sexpr{round(summary(SN_model)$coefficients[2, 1], 2)}\% more students applied, holding all other variables constant. If the school will be able to reduce the cost of living by 1\%, they can expect to receive \Sexpr{-round(summary(SN_model)$coefficients[5, 1], 2)}\% more applications each year.\\
\end{enumerate}


